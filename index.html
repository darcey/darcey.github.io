<!DOCTYPE html>
<html>
<head>
    <title>Darcey Riley</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
    <h1>Darcey Riley</h1>
    <hr>
    <div class="horizontal-block">
        <div class="about-me">
            <h3>About me:</h3>
            <p>I am a 4th-year PhD student in the <a href="https://cse.nd.edu/">Department of Computer Science and Engineering</a> at <a href="https://www.nd.edu/">University of Notre Dame</a>, where I work in <a href="https://www3.nd.edu/~dchiang/">David Chiang</a>'s <a href="https://nlp.nd.edu/">NLP group</a>.</p>
        
            <p>My current research focuses on generation from language models. I'm interested in the entire probability distribution defined by these models, and particularly the ways in which it differs from the "human distribution". Why do language models like to generate overly short or repetitive text? And what can this tell us about how their language processing differs from human language processing? I am also interested in different decoding methods that make it possible to access different regions of the language model's probability distribution.</p>
            
            <p>My roots are in probabilistic modeling and Bayesian epistemology: can we build agents that continually learn about the world by making observations and then updating their probability distributions? What sorts of approximations are needed to make this tractable? What can the process of approximating the ideal Bayesian agent teach us about human thought? Although I am not currently working on this topic, it remains in the back of my thoughts and continues to inform my research perspective.</p>
        </div>
        <div class="headshot-and-contact">
            <img class="headshot-photo" src="headshot.jpg" align="right">
            <ul class="contact">
                <li><b>Email:</b> <a href="mailto:darcey.riley@nd.edu">darcey.riley@nd.edu</a>
                <li><b>Twitter:</b> <a href="https://twitter.com/DarceyNLP">@DarceyNLP</a>
                <li><b>LinkedIn:</b> <a href="https://www.linkedin.com/in/darcey-riley/">Darcey Riley</a>
                <!-- CV -->
                <!-- github? -->
                <!-- Google Scholar? -->
            </ul>
        </div>
    </div>
    <hr>
    <div class="horizontal-block">
        <h3>Publications:</h3>
        <ul>
            <li><b>Darcey Riley</b> and David Chiang. <a href="https://arxiv.org/abs/2210.10817">A Continuum of Generation Tasks for Investigating Length Bias and Degenerate Repetition</a>. To appear at BlackboxNLP 2022.
            <li>David Chiang and <b>Darcey Riley</b>. <a href="https://proceedings.neurips.cc/paper/2020/hash/49ca03822497d26a3943d5084ed59130-Abstract.html">Factor Graph Grammars</a>. In <i>Proc. NeurIPS</i>. 2020.
            <li><b>Darcey Riley</b> and Daniel Gildea. <a href="https://www.aclweb.org/anthology/P12-2060/">Improving the IBM Alignment Models Using Variational Bayes</a>. In <i>Proc. ACL</i>, volume 2, 306-310. 2012.
        </ul>
    </div>
    <hr>
    <div class="horizontal-block">
        <h3>Recorded Talks:</h3>
        <ul>
            <li><a href="https://www.youtube.com/watch?v=gtKpkrSCVh0">Tree Adjoining Grammars and How to Parse Them</a> (talk given at the Formal Languages and Neural Networks (FLaNN) Discord)
            <li><a href="https://www.youtube.com/watch?v=8xhTESwcpkw">Factor Graph Grammars for Probabilistic Modeling</a> (talk given at the <a href="https://compcalc.github.io/">Mila Computational Calculus reading group</a>)
        </ul>
    </div>
    <hr>
    <div class="horizontal-block">
        <h3>Education and work experience:</h3>
        <ul>
            <li>University of Notre Dame, PhD (in progress), Computer Science and Engineering, 2019-present
            <li>Google, Software Engineer, 2016-2019
            <li>Johns Hopkins University, PhD (unfinished), Computer Science, 2012-2015
            <li>University of Rochester, MS, Computer Science, 2010-2012
            <li>University of Rochester, BS, Computer Science, 2008-2012
            <li>University of Rochester, BA, Mathematics, 2008-2012
        </ul>
    </div>
    <hr>
    <div class="horizontal-block">
        <h3>Professional service:</h3>
        <ul>
            <li>2022-2023: Currently serving on Notre Dame CSE's graduate student board.
            <li>2021-2022: I helped to organize Notre Dame's brand new NLP seminar, <a href="https://nlp.nd.edu/nlplus/">NL+</a>.</p>
        </ul>
    </div>
    <hr>
    <div class="horizontal-block">
        <h3>In my spare time...</h3>
            <p>I enjoy dressing up in <a href="halloween.html">CS-themed costumes</a> for Halloween.</p>
            
            <p>My actual proudest NLP accomplishment is this <a href="hmm_sonnet.html">sonnet</a> I once wrote about HMMs.</p>
    </div>
</body>
</html>
